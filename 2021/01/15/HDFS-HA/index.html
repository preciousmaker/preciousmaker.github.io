<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>HDFS HA | my precious</title><meta name="author" content="田一顷"><meta name="copyright" content="田一顷"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="翻译官网https:&#x2F;&#x2F;hadoop.apache.org&#x2F;docs&#x2F;r2.10.1&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;HDFSHighAvailabilityWithQJM.html 生词cluster            集群feature            特征，特点failover        故障转移alternative        备选方案Pr">
<meta property="og:type" content="article">
<meta property="og:title" content="HDFS HA">
<meta property="og:url" content="http://example.com/2021/01/15/HDFS-HA/index.html">
<meta property="og:site_name" content="my precious">
<meta property="og:description" content="翻译官网https:&#x2F;&#x2F;hadoop.apache.org&#x2F;docs&#x2F;r2.10.1&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;HDFSHighAvailabilityWithQJM.html 生词cluster            集群feature            特征，特点failover        故障转移alternative        备选方案Pr">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-01-15T13:07:39.933Z">
<meta property="article:modified_time" content="2021-01-22T11:04:24.451Z">
<meta property="article:author" content="田一顷">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/01/15/HDFS-HA/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-01-22 19:04:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/header.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a></div></div></div><hr/></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">my precious</a></span><span id="menus"><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">HDFS HA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-01-15T13:07:39.933Z" title="发表于 2021-01-15 21:07:39">2021-01-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-22T11:04:24.451Z" title="更新于 2021-01-22 19:04:24">2021-01-22</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>翻译官网<br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></p>
<h1 id="生词"><a href="#生词" class="headerlink" title="生词"></a>生词</h1><p>cluster            集群<br>feature            特征，特点<br>failover        故障转移<br>alternative        备选方案<br>Prior            …事前<br>capable of        能够<br>synchronized    已同步<br>up-to-date        最新的<br>effectively        有效地<br>daemon            守护程序<br>odd number        奇数<br>performs        执行，施行<br>Federation configuration    联邦配置<br>implementation    实作，实施<br>sufficient        足够<br>initiate        发起<br>diagnostic        诊断<br>isolation        隔离<br>corresponds        对应</p>
<p>HDFS High Availability Using the Quorum Journal Manager<br>使用QJM的HDFS高可用</p>
<h1 id="Purpose-目标"><a href="#Purpose-目标" class="headerlink" title="Purpose(目标)"></a>Purpose(目标)</h1><p>This guide provides an overview of the HDFS High Availability (HA) feature and how to configure and manage an HA HDFS cluster, using the Quorum Journal Manager (QJM) feature.<br>本指南概述了HDFS高可用性(HA)功能以及如何使用Quorum Journal Manager(QJM)的功能，配置和管理HA HDFS集群。</p>
<p>This document assumes that the reader has a general understanding of general components and node types in an HDFS cluster. Please refer to the HDFS Architecture guide for details.<br>本文档假定读者对HDFS集群中的常规组件和节点类型有一定的了解，相关信息请参阅HDFS体系结构指南。</p>
<h1 id="Note-Using-the-Quorum-Journal-Manager-or-Conventional-Shared-Storage-注意：使用QJM或者常规共享存储"><a href="#Note-Using-the-Quorum-Journal-Manager-or-Conventional-Shared-Storage-注意：使用QJM或者常规共享存储" class="headerlink" title="Note: Using the Quorum Journal Manager or Conventional Shared Storage(注意：使用QJM或者常规共享存储)"></a>Note: Using the Quorum Journal Manager or Conventional Shared Storage(注意：使用QJM或者常规共享存储)</h1><p>This guide discusses how to configure and use HDFS HA using the Quorum Journal Manager (QJM) to share edit logs between the Active and Standby NameNodes.<br>本指南讨论如何使用QJM配置和使用HDFS HA在active NN和standby NN之间共享编辑日志。</p>
<p>For information on how to configure HDFS HA using NFS for shared storage instead of the QJM, please see this alternative guide..<br>有关如何使用NFS(而不是QJM)用来共享存储来配置HDFS HA的信息，请参阅此替代指南(<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html</a>)</p>
<p>For information on how to configure HDFS HA with Observer NameNode, please see this guide<br>有关如何使用Observer NameNode配置HDFS HA的信息，请参与本指南(<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html</a>)</p>
<h1 id="Background-背景"><a href="#Background-背景" class="headerlink" title="Background(背景)"></a>Background(背景)</h1><p>Prior to Hadoop 2.0.0, the NameNode was a single point of failure (SPOF) in an HDFS cluster.<br>在Hadoop 2.0.0之前，NN在HDFS集群中会发生单点故障(SPOF)</p>
<p>Each cluster had a single NameNode, and if that machine or process became unavailable, the cluster as a whole would be unavailable until the NameNode was either restarted or brought up on a separate machine.<br>每个集群只有一个NN，并且如果该机器或进程不可用，则整个集群将不可用，知道NN重启或在单独的机器上启动。</p>
<p>This impacted the total availability of the HDFS cluster in two major ways:<br>这两个方面影响了HDFS集群的总可用性：</p>
<p>· In the case of an unplanned event such as a machine crash, the cluster would be unavailable until an operator restarted the NameNode.<br>· 如果发生意外事件(例如机器崩溃)，则在操作员重启NN之前，集群将不可用。</p>
<p>· Planned maintenance events such as software or hardware upgrades on the NameNode machine would result in windows of cluster downtime.<br>· 计划内的维护时间，例如NN机器上的软件或硬件升级，将导致集群停机时间的延长。</p>
<p>The HDFS High Availability feature addresses the above problems by providing the option of running two redundant NameNodes in the same cluster in an Active/Passive configuration with a hot standby.<br>HDFS高可用功能通过在具备热备功能的active / standby配置在同一集群中运行两个冗余NN的选项来解决上述问题。</p>
<p>This allows a fast failover to a new NameNode in the case that a machine crashes, or a graceful administrator-initiated failover for the purpose of planned maintenance.<br>这可以在机器崩溃的情况下快速故障转移到新的NN，或者出于计划维护的目的由管理员发起的正常故障转移。</p>
<h1 id="Architecture-结构"><a href="#Architecture-结构" class="headerlink" title="Architecture(结构)"></a>Architecture(结构)</h1><p>In a typical HA cluster, two separate machines are configured as NameNodes.<br>在典型的HA集群中，将两台单独的机器配置为NN</p>
<p>At any point in time, exactly one of the NameNodes is in an Active state, and the other is in a Standby state.<br>在任何时间点，恰好只有其中一个NN处于active状态，另一个处于standby状态</p>
<p>The Active NameNode is responsible for all client operations in the cluster, while the Standby is simply acting as a slave, maintaining enough state to provide a fast failover if necessary.<br>active NN负责集群中的所有客户端操作，而standby NN只是充当附属，并保持足够的状态，以在必要时提供快速故障转移。</p>
<p>In order for the Standby node to keep its state synchronized with the Active node, both nodes communicate with a group of separate daemons called “JournalNodes” (JNs).<br>为了使standby节点保持其状态和active节点同步，两个节点都与称为 JournalNodes(JN) 的一组单独的守护程序进行通信。</p>
<p>When any namespace modification is performed by the Active node, it durably logs a record of the modification to a majority of these JNs.<br>当active节点执行任何命名空间的修改时，它会持久的修改记录，并记录到大多数JN中。</p>
<p>The Standby node is capable of reading the edits from the JNs, and is constantly watching them for changes to the edit log.<br>standby节点能够从JN读取编辑内容，并且不断监控查看编辑日志的更改。</p>
<p>As the Standby Node sees the edits, it applies them to its own namespace.<br>当standby节点看到编辑内容时，会将其应用到自己的命名空间。</p>
<p>In the event of a failover, the Standby will ensure that it has read all of the edits from the JournalNodes before promoting itself to the Active state.<br>当发生故障转移时，standby节点将确保在自身升级到active节点之前，已经从JN读取到所有编辑内容。</p>
<p>This ensures that the namespace state is fully synchronized before a failover occurs.<br>这样可以确保在发生故障转移之前的命名空间状态是完全同步的。</p>
<p>In order to provide a fast failover, it is also necessary that the Standby node have up-to-date information regarding the location of blocks in the cluster.<br>为了提供快速故障转移，standby节点还必须具有有关集群中块位置的最新信息。</p>
<p>In order to achieve this, the DataNodes are configured with the location of both NameNodes, and send block location information and heartbeats to both.<br>为了实现这一点，DN被配置了两个NN的位置，并想两者发送块位置信息和心跳。</p>
<p>It is vital for the correct operation of an HA cluster that only one of the NameNodes be Active at a time.<br>对于HA集群的正常操作至关重要，一直只能有一个NN处于active状态。</p>
<p>Otherwise, the namespace state would quickly diverge between the two, risking data loss or other incorrect results.<br>否则，命名空间状态会在两者之间产生分歧，从而会造成数据丢失或其他不正确结果的风险。</p>
<p>In order to ensure this property and prevent the so-called “split-brain scenario,” the JournalNodes will only ever allow a single NameNode to be a writer at a time.<br>为了确保此属性并且防止所谓的“脑裂”，JN将一次仅允许一个NN成为写入者。</p>
<p>During a failover, the NameNode which is to become active will simply take over the role of writing to the JournalNodes,<br>which will effectively prevent the other NameNode from continuing in the Active state, allowing the new Active to safely proceed with failover.<br>故障转移期间，即将变为active状态的NN仅承担向JN写入数据的角色，<br>这将有效地防止另一个NN继续处于active状态，从而使新的active节点可以安全的进行故障转移。</p>
<h1 id="Hardware-resources-硬件资源"><a href="#Hardware-resources-硬件资源" class="headerlink" title="Hardware resources(硬件资源)"></a>Hardware resources(硬件资源)</h1><p>In order to deploy an HA cluster, you should prepare the following:<br>为了部署高可用性集群，需要准备以下内容：</p>
<p>· <strong>NameNode machines</strong> - the machines on which you run the Active and Standby NameNodes should have equivalent hardware to each other, and equivalent hardware to what would be used in a non-HA cluster.<br>· <strong>NN机器</strong> - 运行active NN和standby NN的具有相同配置的机器，以及与非HA集群中将使用相同的硬件。</p>
<p>· <strong>JournalNode machines</strong> - the machines on which you run the JournalNodes.<br>· <strong>JN机器</strong> - 运行JN的机器</p>
<p>The JournalNode daemon is relatively lightweight, so these daemons may reasonably be collocated on machines with other Hadoop daemons, for example NameNodes, the JobTracker, or the YARN ResourceManager.<br>JN的守护程序相对较轻，因此可以合理的讲这些守护程序与其它Hadoop守护程序(例如NN，JobTracker或YARN RM)配置在同一台机器上。</p>
<p>Note: There must be at least 3 JournalNode daemons, since edit log modifications must be written to a majority of JNs. This will allow the system to tolerate the failure of a single machine.<br>注意：必须至少有3个JN守护程序，因为必须将编辑日志修改写入到大多数JN。这将允许系统容忍单个机器的故障。</p>
<p>You may also run more than 3 JournalNodes, but in order to actually increase the number of failures the system can tolerate, you should run an odd number of JNs, (i.e. 3, 5, 7, etc.).<br>您可能还会运行3个以上的JN，但是为了实际增加系统可容忍的故障数量，应该运行奇数个JN(即3,5,7等)。</p>
<p>Note that when running with N JournalNodes, the system can tolerate at most (N - 1) / 2 failures and continue to function normally.<br>请注意，当N个JN一起运行时，系统最多可以容忍(N-1)/2个故障，并继续正常运行。</p>
<p>Note that, in an HA cluster, the Standby NameNode also performs checkpoints of the namespace state, and thus it is not necessary to run a Secondary NameNode, CheckpointNode, or BackupNode in an HA cluster.<br>请注意，在高可用性集群中，standby NN还会执行命名空间状态的检查点，因此不必在HA集群中运行SNN，CheckpointNode或者BackupNode。</p>
<p>In fact, to do so would be an error. This also allows one who is reconfiguring a non-HA-enabled HDFS cluster to be HA-enabled to reuse the hardware which they had previously dedicated to the Secondary NameNode.<br>实际上，这样做是错误的。这也允许一个重新配置的非HA的HDFS集群的用户启用HA，以重新使用之前专用于SNN的硬件。</p>
<h1 id="Deployment-部署方式"><a href="#Deployment-部署方式" class="headerlink" title="Deployment(部署方式)"></a>Deployment(部署方式)</h1><h2 id="Configuration-overview-配置概述"><a href="#Configuration-overview-配置概述" class="headerlink" title="Configuration overview(配置概述)"></a>Configuration overview(配置概述)</h2><p>Similar to Federation configuration, HA configuration is backward compatible and allows existing single NameNode configurations to work without change.<br>与联邦配置类似，HA配置是向后兼容的，允许现有的单个NN的配置无要修改即可工作。</p>
<p>The new configuration is designed such that all the nodes in the cluster may have the same configuration without the need for deploying different configuration files to different machines based on the type of the node.<br>新的配置设计使集群中的所有节点都可以具有相同的配置，而不需要根据节点类型的不同将不同的配置文件部署到不同的机器上。</p>
<p>Like HDFS Federation, HA clusters reuse the nameservice ID to identify a single HDFS instance that may in fact consist of multiple HA NameNodes.<br>与HDFS联合身份验证一样，HA集群重用了nameservice ID来识别一个HDFS实例，而这个实例实际上可能包含多个HA NN。</p>
<p>In addition, a new abstraction called NameNode ID is added with HA.<br>此外，HA还添加了一个名为NameNode ID的新抽象。</p>
<p>Each distinct NameNode in the cluster has a different NameNode ID to distinguish it. To support a single configuration file for all of the NameNodes, the relevant configuration parameters are suffixed with the <strong>nameservice ID</strong> as well as the <strong>NameNode ID</strong>.<br>集群中的每个不同的NameNode都有一个不同的NameNode ID来区分它。为了支持所有NN的单一配置文件，相关配置参数<strong>nameservice ID</strong>和<strong>NameNode ID</strong>作为后缀。</p>
<h2 id="Configuration-details-配置细节"><a href="#Configuration-details-配置细节" class="headerlink" title="Configuration details(配置细节)"></a>Configuration details(配置细节)</h2><p>To configure HA NameNodes, you must add several configuration options to your <strong>hdfs-site.xml</strong> configuration file.<br>要配置HA NameNode，必须将多个配置选项添加到<strong>hdfs-site.xml</strong>配置文件中。</p>
<p>The order in which you set these configurations is unimportant, but the values you choose for <strong>dfs.nameservices</strong> and <strong>dfs.ha.namenodes.[nameservice ID]</strong> will determine the keys of those that follow.<br>配置这些设置的顺序并不重要，但是为<strong>dfs.nameservices</strong> 和 **dfs.ha.namenodes.[nameservice id]**设置的值将决定后面的键值。</p>
<p>Thus, you should decide on these values before setting the rest of the configuration options.<br>因此，您应该在设置其他配置选项之前确定这些值。</p>
<p>· <strong>dfs.nameservices</strong> - the logical name for this new nameservice<br>· <strong>dfs.nameservices</strong> - 新nameservice的逻辑名称</p>
<p>Choose a logical name for this nameservice, for example “mycluster”, and use this logical name for the value of this config option. The name you choose is arbitrary.<br>It will be used both for configuration and as the authority component of absolute HDFS paths in the cluster.<br>为这个nameservice选择一个逻辑名称，例如“mycluster”，并使用这个逻辑名称作为这个配置选项的值。您选择的名称是任意的。<br>它即可以用于配置，也可以作为集群中HDFS绝对路径的权限组件。</p>
<p><strong>Note:</strong> If you are also using HDFS Federation, this configuration setting should also include the list of other nameservices, HA or otherwise, as a comma-separated list.<br><strong>注意：</strong>如果您还是用HDFS Federation，则此配置设置还应包括其他名称服务(HA或其他)的列表，以逗号分割的列表。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>· <strong>dfs.ha.namenodes.[nameservice ID]</strong> - unique identifiers for each NameNode in the nameservice<br>· <strong>dfs.ha.namenodes.[nameservice ID]</strong> - nameservice中每个NameNode的唯一标识符</p>
<p>Configure with a list of comma-separated NameNode IDs.This will be used by DataNodes to determine all the NameNodes in the cluster.<br>配置一个以逗号分隔的NameNode ID列表，DataNode将使用它来确定集群中的所有NameNode。</p>
<p>For example, if you used “mycluster” as the nameservice ID previously, and you wanted to use “nn1” and “nn2” as the individual IDs of the NameNodes, you would configure this as such:<br>例如，如果你以前使用“mycluster”作为nameservice ID，想使用“nn1”和“nn2”作为NameNode的单独ID，你可以这样配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">```	</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Note: **Currently, only a maximum of two NameNodes may be configured per nameservice.</span><br><span class="line">**注意：**目前每个nameservice最多只能配置两个NameNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">· **dfs.namenode.rpc-address.[nameservice ID].[name node ID]** - the fully-qualified RPC address for each NameNode to listen on</span><br><span class="line">· **dfs.namenode.rpc-address.[nameservice ID].[name node ID]** - 每个NameNode监听的全限定RPC地址</span><br><span class="line"></span><br><span class="line">For both of the previously-configured NameNode IDs, set the full address and IPC port of the NameNode processs. Note that this results in two separate configuration options. For example:</span><br><span class="line">对于前面配置的两个NameNode ID，需要设置NameNode进程的完整地址和IPC端口。注意，这会导致两个单独的配置选项。例如：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>machine1.example.com:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>machine2.example.com:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p><strong>Note: **You may similarly configure the “servicerpc-address” setting if you so desire.<br>**注意：</strong>如果您愿意，您也可以同样配置“servicerpc-address”设置。</p>
<p>· <strong>dfs.namenode.http-address.[nameservice ID].[name node ID]</strong> - the fully-qualified HTTP address for each NameNode to listen on<br>· <strong>dfs.namenode.http-address.[nameservice ID].[name node ID]</strong> - 每个NameNode监听的完全限定HTTP地址</p>
<p>Similarly to rpc-address above, set the addresses for both NameNodes’ HTTP servers to listen on. For example:<br>与上面的rpc-address类似，设置两个NameNode的HTTP服务器要监听的地址。例如：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>machine1.example.com:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>machine2.example.com:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>Note: **If you have Hadoop’s security features enabled, you should also set the https-address similarly for each NameNode.<br>**注意：</strong>如果启用了Hadoop的安全特性，还应该为每个NameNode设置类似的http-address</p>
<p>· <strong>dfs.namenode.shared.edits.dir</strong> - the URI which identifies the group of JNs where the NameNodes will write/read edits<br>· <strong>dfs.namenode.shared.edits.dir</strong> - 标识NameNode将要读写的JN组的URI</p>
<p>This is where one configures the addresses of the JournalNodes which provide the shared edits storage,<br>written to by the Active nameNode and read by the Standby NameNode to stay up-to-date with all the file system changes the Active NameNode makes.<br>在这里配置提供共享编辑存储的JN的地址，<br>由active NN写入，由standby NN读取，以保持对active NN所做的所有文件系统更改的最新信息。</p>
<p>Though you must specify several JournalNode addresses, <strong>you should only configure one of these URIs.</strong><br>尽管必须制定多个JN的地址，<strong>但仅应配置其中一个URI。</strong></p>
<p>The URI should be of the form: qjournal://<em>host1:port1</em>;<em>host2:port2</em>;<em>host3:port3</em>/<em>journalId</em>.<br>URI应该是这样的形式：qjournal://<em>host1:port1</em>;<em>host2:port2</em>;<em>host3:port3</em>/<em>journalId</em>.</p>
<p>The Journal ID is a unique identifier for this nameservice, which allows a single set of JournalNodes to provide storage for multiple federated namesystems.<br>Journal ID是此nameservice的唯一标识符，他允许单个JN为多个联合名称系统提供存储。</p>
<p>Though not a requirement, it’s a good idea to reuse the nameservice ID for the journal identifier.<br>尽管不是必需的，但最好将nameservice ID用作Journal标识符。</p>
<p>For example, if the JournalNodes for this cluster were running on the machines “node1.example.com”, “node2.example.com”, and “node3.example.com” and the nameservice ID were “mycluster”,<br>you would use the following as the value for this setting (the default port for the JournalNode is 8485):<br>例如，如果此集群的JN在机器“node1.example.com”, “node2.example.com”, and “node3.example.com”上运行，并且nameservice ID为“mycluster”，<br>则应试用一下为该设置的值(JN的默认端口号为8485)：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">```	</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">· **dfs.client.failover.proxy.provider.[nameservice ID]** - the Java class that HDFS clients use to contact the Active NameNode</span><br><span class="line">· **dfs.client.failover.proxy.provider.[nameservice ID]** - HDFS客户端用来链接active NN的Java类</span><br><span class="line"></span><br><span class="line">Configure the name of the Java class which will be used by the DFS Client to determine which NameNode is the current Active, and therefore which NameNode is currently serving client requests. </span><br><span class="line">配置Java类的名称，DFS客户端将使用该Java类来确定哪个NN当前是active，从而确定哪个NN当前正服务于客户端请求。</span><br><span class="line"></span><br><span class="line">The two implementations which currently ship with Hadoop are the ConfiguredFailoverProxyProvider and the RequestHedgingProxyProvider </span><br><span class="line">(which, for the first call, concurrently invokes all namenodes to determine the active one, and on subsequent requests, invokes the active namenode until a fail-over happens), </span><br><span class="line">so use one of these unless you are using a custom proxy provider. For example:</span><br><span class="line">Hadoop当前附带的两个实现是ConfiguredFailoverProxyProvider and the RequestHedgingProxyProvider</span><br><span class="line">(对于第一个调用，他会同时调用所有NN以确定active NN，并在后续请求时调用active NN直到发生故障转移)</span><br><span class="line">因此，除非您使用自定义代理提供程序，否则请使用其中之一。例如：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">```	</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">· **dfs.ha.fencing.methods** - a list of scripts or Java classes which will be used to fence the Active NameNode during a failover</span><br><span class="line">· **dfs.ha.fencing.methods** - 故障转移期间将用于隔离active NN的脚本或Java类的列表</span><br><span class="line"></span><br><span class="line">It is desirable for correctness of the system that only one NameNode be in the Active state at any given time.</span><br><span class="line">为了保证系统的正确性，在任何给定时间只有一个NN处于active状态。</span><br><span class="line"></span><br><span class="line">**Importantly, when using the Quorum Journal Manager, only one NameNode will ever be allowed to write to the JournalNodes, so there is no potential for corrupting the file system metadata from a split-brain scenario.**</span><br><span class="line">**重要的是，使用Quorum Journal Manager时，将只允许一个NN写入JN，因此不会因脑裂情况而损坏文件系统元数据**</span><br><span class="line"></span><br><span class="line">However, when a failover occurs, it is still possible that the previous Active NameNode could serve read requests to clients, which may be out of date until that NameNode shuts down when trying to write to the JournalNodes.</span><br><span class="line">但是，当发生故障转移时，以前的active NN仍可能会向客户端提供读取请求，这可能已过期，直到该NN在尝试写入JN时关闭为止。</span><br><span class="line"></span><br><span class="line">For this reason, it is still desirable to configure some fencing methods even when using the Quorum Journal Manager. </span><br><span class="line">因此，即使使用QJM，仍然需要配置一些fencing方法。</span><br><span class="line"></span><br><span class="line">However, to improve the availability of the system in the event the fencing mechanisms fail, it is advisable to configure a fencing method which is guaranteed to return success as the last fencing method in the list.</span><br><span class="line">但是，为了在防护机制失败的情况下提高系统的可用性，建议配置一种fencing方法，它可以保证返回成功，作为列表中的最后一个fencing方法。</span><br><span class="line"></span><br><span class="line">Note that if you choose to use no actual fencing methods, you still must configure something for this setting, for example “shell(/bin/true)”.</span><br><span class="line">请注意，如果您选择不使用实际的fencing方法，仍必须为此设置配置一些内容，例如 “shell(/bin/true)”。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The fencing methods used during a failover are configured as a carriage-return-separated list, which will be attempted in order until one indicates that fencing has succeeded. </span><br><span class="line">故障转移过程中使用的fencing方法被配置为一个回车分隔列表，将依次尝试，直到其中一个表明隔离已经成功。</span><br><span class="line"></span><br><span class="line">There are two methods which ship with Hadoop: shell and sshfence. For information on implementing your own custom fencing method, see the org.apache.hadoop.ha.NodeFencer class.</span><br><span class="line">Hadoop附带了两种方法:shell和sshfence。有关实现您自己的自定义fencing方法的信息，请参阅org.apache.hadoop.ha。NodeFencer类。</span><br><span class="line"></span><br><span class="line">**sshfence** - SSH to the Active NameNode and kill the process</span><br><span class="line">**sshfence** - SSH跳转到active NN并终止进程</span><br><span class="line"></span><br><span class="line">The sshfence option SSHes to the target node and uses fuser to kill the process listening on the service’s TCP port. </span><br><span class="line">sshfence选项通过ssh连接到目标节点，并使用fuser终止侦听服务TCP端口的进程。</span><br><span class="line"></span><br><span class="line">In order for this fencing option to work, it must be able to SSH to the target node without providing a passphrase.</span><br><span class="line">为了使该fencing选项起作用，他必须能够在不提供密码的情况下通过SSH到目标节点。</span><br><span class="line"></span><br><span class="line">Thus, one must also configure the dfs.ha.fencing.ssh.private-key-files option, which is a comma-separated list of SSH private key files. For example:</span><br><span class="line">因此，还必须配置dfs.ha.fencing.ssh.private-key-files选项，该选项是用逗号分隔的SSH私钥文件列表。例如：</span><br><span class="line"></span><br><span class="line">```xml</span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/exampleuser/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Optionally, one may configure a non-standard username or port to perform the SSH.<br>(可选)可以配置非标准用户名和端口以执行SSH</p>
<p>One may also configure a timeout, in milliseconds, for the SSH, after which this fencing method will be considered to have failed. It may be configured like so:<br>您还可以为SSH配置一个以毫秒为单位的超市，以后该fencing方法将被视为失败。可以这样配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence([[username][:port]])<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p><strong>shell</strong> - run an arbitrary shell command to fence the Active NameNode<br><strong>shell</strong> - 运行任意的shell命令以隔离active NN</p>
<p>The shell fencing method runs an arbitrary shell command. It may be configured like so:<br>shell fencing方法运行一个任意的shell命令。可以这样配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>shell(/path/to/my/script.sh arg1 arg2 ...)<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">```	</span><br><span class="line"></span><br><span class="line">The string between ‘(’ and ‘)’ is passed directly to a bash shell and may not include any closing parentheses.</span><br><span class="line">‘(’ and ‘)’之间的字符串将直接传递到bash shell，并且可能不包含任何右括号。</span><br><span class="line"></span><br><span class="line">The shell command will be run with an environment set up to contain all of the current Hadoop configuration variables, with the ‘_’ character replacing any ‘.’ characters in the configuration keys.</span><br><span class="line">shell命令将在一个设置为包含所有当前Hadoop配置变量的环境下运行，并用‘_’字符替换配置键中的任何‘.’字符。</span><br><span class="line"></span><br><span class="line">The configuration used has already had any namenode-specific configurations promoted to their generic forms – </span><br><span class="line">for example dfs_namenode_rpc-address will contain the RPC address of the target node, </span><br><span class="line">even though the configuration may specify that variable as dfs.namenode.rpc-address.ns1.nn1.</span><br><span class="line">所使用的配置已经将任何特定于名称节点的配置提升为通用形式，</span><br><span class="line">例如dfs_namenode_rpc-address将包含目标节点的RPC地址，</span><br><span class="line">即使改配置可以将该变量指定为dfs.namenode.rpc-address.ns1.nn1.</span><br><span class="line"></span><br><span class="line">Additionally, the following variables referring to the target node to be fenced are also available:</span><br><span class="line">此外，还提供以下变量，这些变量引用要隔离的目标节点：</span><br><span class="line"></span><br><span class="line">```markdown	</span><br><span class="line">	| 变量名 | 作用 |</span><br><span class="line">	|---------|---------|</span><br><span class="line">	$target_host | hostname of the node to be fenced</span><br><span class="line">	$target_port | IPC port of the node to be fenced</span><br><span class="line">	$target_address | the above two, combined as host:port</span><br><span class="line">	$target_nameserviceid | the nameservice ID of the NN to be fenced</span><br><span class="line">	$target_namenodeid | the namenode ID of the NN to be fenced</span><br></pre></td></tr></table></figure>

<p>These environment variables may also be used as substitutions in the shell command itself. For example:<br>这些环境变量也可以在shell命令本身中用作代替，例如：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>shell(/path/to/my/script.sh --nameservice=$target_nameserviceid $target_host:$target_port)<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>If the shell command returns an exit code of 0, the fencing is determined to be successful.<br>If it returns any other exit code, the fencing was not successful and the next fencing method in the list will be attempted.<br>如果shell命令返回退出代码0，则确定fencing成功。<br>如果返回任何其他退出代码，则fencing不成功，并且将尝试列表中的下一个fencing方法。</p>
<p>Note: This fencing method does not implement any timeout.<br>If timeouts are necessary, they should be implemented in the shell script itself (eg by forking a subshell to kill its parent in some number of seconds).<br>注意：此fnecing方法不会实现任何超时。<br>如果需要超时，则应在shell脚本本身中实现超时（例如，通过分叉子shell在几秒钟内杀死其父shell）。</p>
<p>· <strong>fs.defaultFS</strong> - the default path prefix used by the Hadoop FS client when none is given<br>· <strong>fs.defaultFS</strong> - 没有给出Hadooop FS客户端使用的默认路径前缀。</p>
<p>Optionally, you may now configure the default path for Hadoop clients to use the new HA-enabled logical URI.<br>(可选)您现在可以配置Hadoop客户端的默认路径，以启用新的HA的逻辑URI。</p>
<p>If you used “mycluster” as the nameservice ID earlier, this will be the value of the authority portion of all of your HDFS paths. This may be configured like so, in your core-site.xml file:<br>如果前面使用“mycluster”作为nameservice ID，那么这将是所有HDFS路径的授权部分的值。在你的core-site.xml文件中可以这样配置:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>· <strong>dfs.journalnode.edits.dir</strong> - the path where the JournalNode daemon will store its local state<br>· <strong>dfs.journalnode.edits.dir</strong> - JN守护程序将存储其本地状态的路径</p>
<p>This is the absolute path on the JournalNode machines where the edits and other local state used by the JNs will be stored.<br>这是JournalNode机器上的绝对路径，JN使用的编辑和其他本地状态将存储在这里。</p>
<p>You may only use a single path for this configuration.<br>对于此配置，您只能使用单个路径。</p>
<p>Redundancy for this data is provided by running multiple separate JournalNodes, or by configuring this directory on a locally-attached RAID array. For example:<br>通过运行多个独立的JN，或在本地连接的RAID阵列上配置此目录，可以提供此数据的冗余。例如:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/path/to/journal/node/local/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<h2 id="Deployment-details-部署细节"><a href="#Deployment-details-部署细节" class="headerlink" title="Deployment details(部署细节)"></a>Deployment details(部署细节)</h2><p>After all of the necessary configuration options have been set, you must start the JournalNode daemons on the set of machines where they will run.<br>设置完所有必需的配置选项后，必须在将要运行它们的机器上启动JN守护程序。</p>
<p>This can be done by running the command “hadoop-daemon.sh start journalnode” and waiting for the daemon to start on each of the relevant machines.<br>这可以通过运行命令“ hadoop-daemon.sh start journalnode”并等待守护程序在每台相关机器上启动来完成。</p>
<p>Once the JournalNodes have been started, one must initially synchronize the two HA NameNodes’ on-disk metadata.<br>一旦启动JournalNode，便必须首先同步两个HA NameNode的磁盘元数据。</p>
<p>· If you are setting up a fresh HDFS cluster, you should first run the format command (hdfs namenode -format) on one of NameNodes.<br>· 如果要设置新的HDFS集群，则应首先在其中一个NameNode上运行format命令（hdfs namenode -format）。</p>
<p>· If you have already formatted the NameNode, or are converting a non-HA-enabled cluster to be HA-enabled,<br>you should now copy over the contents of your NameNode metadata directories to the other, unformatted NameNode by running the command “hdfs namenode -bootstrapStandby” on the unformatted NameNode.<br>Running this command will also ensure that the JournalNodes (as configured by dfs.namenode.shared.edits.dir) contain sufficient edits transactions to be able to start both NameNodes.<br>如果您已经格式化了NN，或者正在将未启用的HA集群转换为启用HA的集群，<br>您现在应该通过在未格式化的NN上运行命令“hdfs namenode -bootstrapStandby”，将NN元数据目录的内容复制到另一个未格式化的NN。<br>运行此命令还将确保JN（由dfs.namenode.shared.edits.dir配置）包含足够的edits事务以能够启动两个NameNode。</p>
<p>· If you are converting a non-HA NameNode to be HA, you should run the command “hdfs namenode -initializeSharedEdits”, which will initialize the JournalNodes with the edits data from the local NameNode edits directories.<br>· 如果要将非HA NameNode转换为HA，则应运行命令“ hdfs namenode -initializeSharedEdits”，该命令将使用本地NameNode edits目录中的edits数据初始化JournalNode。</p>
<p>At this point you may start both of your HA NameNodes as you normally would start a NameNode.<br>此时，您可以像正常启动NameNode一样启动两个HA NameNode。</p>
<p>You can visit each of the NameNodes’ web pages separately by browsing to their configured HTTP addresses.<br>您可以通过浏览到每个NameNode配置的HTTP地址来分别访问它们。</p>
<p>You should notice that next to the configured address will be the HA state of the NameNode (either “standby” or “active”.) Whenever an HA NameNode starts, it is initially in the Standby state.<br>您应该注意到，配置的地址旁边将是NameNode的HA状态（“standby”或“active”）。每当HA NameNode启动时，它最初都处于Standby状态。</p>
<h2 id="Administrative-commands-管理命令"><a href="#Administrative-commands-管理命令" class="headerlink" title="Administrative commands(管理命令)"></a>Administrative commands(管理命令)</h2><p>Now that your HA NameNodes are configured and started, you will have access to some additional commands to administer your HA HDFS cluster.<br>现在已经配置并启动了HA namenode，您可以访问一些额外的命令来管理HA HDFS集群。</p>
<p>Specifically, you should familiarize yourself with all of the subcommands of the “hdfs haadmin” command. Running this command without any additional arguments will display the following usage information:<br>具体来说，您应该熟悉“hdfs haadmin”命令的所有子命令。不带任何附加参数的命令将显示如下使用信息:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Usage: haadmin</span><br><span class="line">    [-transitionToActive &lt;serviceId&gt;]</span><br><span class="line">    [-transitionToStandby &lt;serviceId&gt;]</span><br><span class="line">    [-failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;]</span><br><span class="line">    [-getServiceState &lt;serviceId&gt;]</span><br><span class="line">    [-getAllServiceState]</span><br><span class="line">    [-checkHealth &lt;serviceId&gt;]</span><br><span class="line">    [-help &lt;command&gt;]</span><br></pre></td></tr></table></figure>

<p>This guide describes high-level uses of each of these subcommands. For specific usage information of each subcommand, you should run “hdfs haadmin -help <command>”.<br>本指南描述了这些子命令的高级用法。对于每个子命令的具体使用信息，可以执行“hdfs haadmin -help ”。</p>
<p>· <strong>transitionToActive</strong> and <strong>transitionToStandby</strong> - transition the state of the given NameNode to Active or Standby<br>· <strong>transitionToActive</strong> and <strong>transitionToStandby</strong> - 将给定NameNode的状态转换为Active或Standby</p>
<p>These subcommands cause a given NameNode to transition to the Active or Standby state, respectively.<br>这些子命令使给定的NN分别转换为active或standby状态。</p>
<p>** These commands do not attempt to perform any fencing, and thus should rarely be used.**<br>这些命令不会尝试执行任何fencing，因此应很少使用。</p>
<p>Instead, one should almost always prefer to use the “hdfs haadmin -failover” subcommand.<br>相反，几乎应该总是更应该使用“hdfs haadmin -failover”命令。</p>
<p>· <strong>failover</strong> - initiate a failover between two NameNodes<br>· <strong>failover</strong> - 在两个NN之间启动故障转移</p>
<p>This subcommand causes a failover from the first provided NameNode to the second.<br>此子命令导致从第一个提供的NameNode到第二个的NameNode故障转移。</p>
<p>If the first NameNode is in the Standby state, this command simply transitions the second to the Active state without error.<br>如果第一个NameNode处于Standby状态，则此命令仅将第二个NameNode转换为Active状态而不会出现错误。</p>
<p>If the first NameNode is in the Active state, an attempt will be made to gracefully transition it to the Standby state.<br>If this fails, the fencing methods (as configured by dfs.ha.fencing.methods) will be attempted in order until one succeeds.<br>如果第一个NameNode处于活动状态，则将尝试将其优雅地转换为Standby状态。<br>如果失败了，依次尝试fencing方法（由dfs.ha.fencing.methods配置），直到成功为止。</p>
<p>Only after this process will the second NameNode be transitioned to the Active state.<br>仅在此过程之后，第二个NameNode才会转换为Active状态。</p>
<p>If no fencing method succeeds, the second NameNode will not be transitioned to the Active state, and an error will be returned.<br>如果没有成功的fencing方法，则第二个NameNode将不会转换为Active状态，并且将返回错误。</p>
<p>· <strong>getServiceState</strong> - determine whether the given NameNode is Active or Standby<br>· <strong>getServiceState</strong> - 确定给定的NameNode是活动的还是备用的</p>
<p>Connect to the provided NameNode to determine its current state, printing either “standby” or “active” to STDOUT appropriately.<br>连接到提供的NameNode以确定其当前状态，并在STDOUT上适当打印“standby”或“active”。</p>
<p>This subcommand might be used by cron jobs or monitoring scripts which need to behave differently based on whether the NameNode is currently Active or Standby.<br>cron作业或监视脚本可能会使用此子命令，它们需要根据NameNode当前是活动的还是备用的而采取不同的行为。</p>
<p>· <strong>getAllServiceState</strong> - returns the state of all the NameNodes<br>· <strong>getAllServiceState</strong> - 返回所有NameNode的状态</p>
<p>Connect to the configured NameNodes to determine the current state, print either “standby” or “active” to STDOUT appropriately.<br>连接到提供的NameNode以确定其当前状态，并在STDOUT上适当打印“standby”或“active”。</p>
<p>· <strong>checkHealth</strong> - check the health of the given NameNode<br>· <strong>checkHealth</strong> - 检查给定NameNode的运行状况</p>
<p>Connect to the provided NameNode to check its health.<br>连接到提供的NameNode来检查其运行状况。</p>
<p>The NameNode is capable of performing some diagnostics on itself, including checking if internal services are running as expected.<br>NameNode能够对其自身执行一些诊断，包括检查内部服务是否按预期运行。</p>
<p>This command will return 0 if the NameNode is healthy, non-zero otherwise. One might use this command for monitoring purposes.<br>如果NameNode正常运行，该命令将返回0，否则将返回非0。可以将此命令用于监视目的。</p>
<p>Note: This is not yet implemented, and at present will always return success, unless the given NameNode is completely down.<br>注意：这尚未实现，除非给定的NameNode完全关闭，否则当前将始终返回成功。</p>
<h2 id="Load-Balancer-Setup-负载均衡器设置"><a href="#Load-Balancer-Setup-负载均衡器设置" class="headerlink" title="Load Balancer Setup(负载均衡器设置)"></a>Load Balancer Setup(负载均衡器设置)</h2><p>If you are running a set of NameNodes behind a Load Balancer (e.g. Azure or AWS ) and would like the Load Balancer to point to the active NN,<br>you can use the /isActive HTTP endpoint as a health probe. <a target="_blank" rel="noopener" href="http://nn_hostname/isActive">http://NN_HOSTNAME/isActive</a> will return a 200 status code response if the NN is in Active HA State, 405 otherwise.<br>如果您正在负载平衡器（例如Azure或AWS）后面运行一组NameNode，并且希望负载平衡器指向活动的NN，<br>您可以将/ isActive HTTP端点用作运行状况探针。 如果NN处于活动HA状态，则<a target="_blank" rel="noopener" href="http://nn_hostname/isActive%E5%B0%86%E8%BF%94%E5%9B%9E200%E7%8A%B6%E6%80%81%E4%BB%A3%E7%A0%81%E5%93%8D%E5%BA%94%EF%BC%8C%E5%90%A6%E5%88%99%E8%BF%94%E5%9B%9E405%E3%80%82">http://NN_HOSTNAME/isActive将返回200状态代码响应，否则返回405。</a></p>
<h2 id="In-Progress-Edit-Log-Tailing-正在进行的编辑日志尾"><a href="#In-Progress-Edit-Log-Tailing-正在进行的编辑日志尾" class="headerlink" title="In-Progress Edit Log Tailing(正在进行的编辑日志尾)"></a>In-Progress Edit Log Tailing(正在进行的编辑日志尾)</h2><p>Under the default settings, the Standby NameNode will only apply edits that are present in an edit log segments which has been finalized.<br>在默认设置下，Standby NameNode将仅应用已完成的编辑日志段中存在的编辑。</p>
<p>If it is desirable to have a Standby NameNode which has more up-to-date namespace information, it is possible to enable tailing of in-progress edit segments.<br>如果希望具有一个具有最新名称空间信息的Standby NameNode，则可以启用正在进行的编辑段的尾部处理。</p>
<p>This setting will attempt to fetch edits from an in-memory cache on the JournalNodes and can reduce the lag time before a transaction is applied on the Standby NameNode to the order of milliseconds.<br>此设置将尝试从JournalNode上的内存高速缓存中获取编辑，并且可以将在Standby NameNode上应用事务之前的延迟时间减少到毫秒量级。</p>
<p>If an edit cannot be served from the cache, the Standby will still be able to retrieve it, but the lag time will be much longer. The relevant configurations are:<br>如果无法从缓存中进行编辑，则备用数据库仍可以检索它，但是延迟时间会更长。 相关配置为：</p>
<p>· <strong>dfs.ha.tail-edits.in-progress</strong> - Whether or not to enable tailing on in-progress edits logs. This will also enable the in-memory edit cache on the JournalNodes. Disabled by default.<br>· <strong>dfs.ha.tail-edits.in-progress</strong> - 是否在进行中的编辑日志中启用拖尾。 这还将在JournalNodes上启用内存中编辑缓存。 默认禁用。</p>
<p>· <strong>dfs.journalnode.edit-cache-size.bytes</strong> - The size of the in-memory cache of edits on the JournalNode.<br>· <strong>dfs.journalnode.edit-cache-size.bytes</strong> - JournalNode上的内存中编辑缓存的大小。</p>
<p>Edits take around 200 bytes each in a typical environment, so, for example, the default of 1048576 (1MB) can hold around 5000 transactions.<br>在典型的环境中，每个编辑占用大约200个字节，因此，例如，默认值1048576（1MB）可以容纳大约5000个事务。</p>
<p>It is recommended to monitor the JournalNode metrics RpcRequestCacheMissAmountNumMisses and RpcRequestCacheMissAmountAvgTxns,<br>建议监视JournalNode指标RpcRequestCacheMissAmountNumMisses和RpcRequestCacheMissAmountAvgTxns，</p>
<p>which respectively count the number of requests unable to be served by the cache,<br>分别计算缓存无法处理的请求数，</p>
<p>and the extra number of transactions which would have needed to have been in the cache for the request to succeed.<br>以及为了使请求成功而需要在缓存中进行的额外事务数量。</p>
<p>For example, if a request attempted to fetch edits starting at transaction ID 10, but the oldest data in the cache was at transaction ID 20, a value of 10 would be added to the average.<br>例如，如果请求尝试从事务ID 10开始获取编辑，但是缓存中最旧的数据在事务ID 20，则将平均值添加10。</p>
<p>This feature is primarily useful in conjunction with the Standby/Observer Read feature.<br>该功能主要与“Standby/Observer”功能结合使用。</p>
<p>Using this feature, read requests can be serviced from non-active NameNodes;<br>使用此功能，可以从非活动的NameNode服务读取请求；</p>
<p>thus tailing in-progress edits provides these nodes with the ability to serve requests with data which is much more fresh.<br>因此，进行中的后期编辑为这些节点提供了为请求提供最新数据的能力。</p>
<p>See the Apache JIRA ticket HDFS-12943 for more information on this feature.<br>有关此功能的更多信息，请参见Apache JIRA票证HDFS-12943。</p>
<h1 id="Automatic-Failover-自动故障转移"><a href="#Automatic-Failover-自动故障转移" class="headerlink" title="Automatic Failover(自动故障转移)"></a>Automatic Failover(自动故障转移)</h1><h2 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction(介绍)"></a>Introduction(介绍)</h2><p>The above sections describe how to configure manual failover.<br>以上各节描述了如何配置手动故障转移。</p>
<p>In that mode, the system will not automatically trigger a failover from the active to the standby NameNode, even if the active node has failed.<br>在该模式下，即使active节点发生故障，系统也不会自动触发从active NN到standby NN的故障转移。</p>
<p>This section describes how to configure and deploy automatic failover.<br>本节介绍如何配置和部署自动故障转移。</p>
<h2 id="Components-组件"><a href="#Components-组件" class="headerlink" title="Components(组件)"></a>Components(组件)</h2><p>Automatic failover adds two new components to an HDFS deployment: a ZooKeeper quorum, and the ZKFailoverController <strong>process</strong> (abbreviated as ZKFC).<br>自动故障转移为HDFS部署添加了两个新组件：ZooKeeper quoru 和 ZKFailoverController<strong>进程</strong>（缩写为ZKFC）</p>
<p>Apache ZooKeeper is a highly available service for maintaining small amounts of coordination data, notifying clients of changes in that data, and monitoring clients for failures.<br>Apache ZooKeeper是一个高可用的服务，用于维护少量的协调数据，通知客户端数据的变化，并监控客户端故障。</p>
<p>The implementation of automatic HDFS failover relies on ZooKeeper for the following things:<br>HDFS自动故障转移的实现依赖ZooKeeper进行以下操作：</p>
<p>· <strong>Failure detection</strong> - each of the NameNode machines in the cluster maintains a persistent session in ZooKeeper.<br>If the machine crashes, the ZooKeeper session will expire, notifying the other NameNode that a failover should be triggered.<br>· <strong>故障检测</strong> - 集群中的每个NameNode机器在ZooKeeper中维护一个持久会话。<br>如果机器崩溃了，ZooKeeper会话将过期，通知另一个NameNode应该触发故障转移。</p>
<p>· <strong>Active NameNode election</strong> - ZooKeeper provides a simple mechanism to exclusively elect a node as active.<br>If the current active NameNode crashes, another node may take a special exclusive lock in ZooKeeper indicating that it should become the next active.<br>· <strong>active NN选举</strong> - ZooKeeper提供了一种简单的机制来专门选择一个节点为active节点。<br>如果当前active NN崩溃，则另一个节点可能会在ZooKeeper中采取特殊的排他锁，指示它应该成为下一个active NN。</p>
<p>The ZKFailoverController (ZKFC) is a new component which is a ZooKeeper client which also monitors and manages the state of the NameNode.<br>Each of the machines which runs a NameNode also runs a ZKFC, and that ZKFC is responsible for:<br>ZKFailoverController（ZKFC）是一个新组件，它是一个ZooKeeper客户端，它还监视和管理NameNode的状态。<br>运行NameNode的每台计算机也都运行ZKFC，该ZKFC负责：</p>
<p>· <strong>Health monitoring</strong> - the ZKFC pings its local NameNode on a periodic basis with a health-check command.<br> So long as the NameNode responds in a timely fashion with a healthy status, the ZKFC considers the node healthy.<br> If the node has crashed, frozen, or otherwise entered an unhealthy state, the health monitor will mark it as unhealthy.<br>· <strong>运行状况监视</strong> - ZKFC使用运行状况检查命令定期ping通其本地NameNode。<br>只要NameNode以健康状态及时响应，ZKFC就会认为该节点是健康的。<br>如果该节点已崩溃，冻结或以其他方式进入不正常状态，则运行状况监视器将其标记为不正常。</p>
<p>· <strong>ZooKeeper session management</strong> - when the local NameNode is healthy, the ZKFC holds a session open in ZooKeeper.<br> If the local NameNode is active, it also holds a special “lock” znode. This lock uses ZooKeeper’s support for “ephemeral” nodes;<br> if the session expires, the lock node will be automatically deleted.<br>· ** ZooKeeper会话管理** – 当本地NameNode运行状况良好时，ZKFC会在ZooKeeper中保持打开的会话。<br>如果本地NameNode处于active状态，则它还将持有一个特殊的“lock” znode。此锁使用ZooKeeper对“ephemeral”节点的支持；<br>如果会话期满，将自动删除锁定节点。</p>
<p>· <strong>ZooKeeper-based election</strong> -  if the local NameNode is healthy, and the ZKFC sees that no other node currently holds the lock znode, it will itself try to acquire the lock.<br>If it succeeds, then it has “won the election”, and is responsible for running a failover to make its local NameNode active.<br>The failover process is similar to the manual failover described above: first, the previous active is fenced if necessary, and then the local NameNode transitions to active state.<br>· <strong>基于ZooKeeper的选举</strong> - 如果本地NameNode状况良好，并且ZKFC看到当前没有其他节点持有锁znode，则它本身将尝试获取该锁。<br>如果成功，则表明它“赢得了选举”，并负责运行故障转移以使其本地NameNode处于active状态。<br>故障转移过程类似于上述的手动故障转移：首先，如有必要，将先前的active节点隔离，然后将本地NameNode转换为active状态。</p>
<p>For more details on the design of automatic failover, refer to the design document attached to HDFS-2185 on the Apache HDFS JIRA.<br>有关自动故障转移设计的更多详细信息，请参阅Apache HDFS JIRA上HDFS-2185附带的设计文档。</p>
<h2 id="Deploying-ZooKeeper-部署ZooKeeper"><a href="#Deploying-ZooKeeper-部署ZooKeeper" class="headerlink" title="Deploying ZooKeeper(部署ZooKeeper)"></a>Deploying ZooKeeper(部署ZooKeeper)</h2><p>In a typical deployment, ZooKeeper daemons are configured to run on three or five nodes.<br>在典型的部署中，ZooKeeper守护程序被配置为在三个或五个节点上运行。</p>
<p>Since ZooKeeper itself has light resource requirements, it is acceptable to collocate the ZooKeeper nodes on the same hardware as the HDFS NameNode and Standby Node.<br>由于ZooKeeper本身对资源的要求较低，所以可以将ZooKeeper节点与HDFS的NameNode和备节点配置在同一硬件上。</p>
<p>Many operators choose to deploy the third ZooKeeper process on the same node as the YARN ResourceManager.<br>很多运营商选择将第三个ZooKeeper进程部署在YARN ResourceManager的同一节点上。</p>
<p>It is advisable to configure the ZooKeeper nodes to store their data on separate disk drives from the HDFS metadata for best performance and isolation.<br>建议将ZooKeeper节点的数据存储在与HDFS元数据分离的磁盘驱动器上，以获得最佳的性能和隔离。</p>
<p>The setup of ZooKeeper is out of scope for this document.<br>ZooKeeper的设置超出了本文档的范围。</p>
<p>We will assume that you have set up a ZooKeeper cluster running on three or more nodes, and have verified its correct operation by connecting using the ZK CLI.<br>我们假设您已经在三个或三个以上的节点上建立了一个ZK集群，并通过ZK CLI连接验证了它的正确操作。</p>
<h2 id="Before-you-begin-在你开始之前"><a href="#Before-you-begin-在你开始之前" class="headerlink" title="Before you begin(在你开始之前)"></a>Before you begin(在你开始之前)</h2><p>Before you begin configuring automatic failover, you should shut down your cluster.<br>在开始配置自动故障转移之前，应该关闭集群。</p>
<p>It is not currently possible to transition from a manual failover setup to an automatic failover setup while the cluster is running.<br>在集群运行时，目前不可能从手动故障转移转换为自动故障转移。</p>
<h2 id="Configuring-automatic-failover-配置自动故障转移"><a href="#Configuring-automatic-failover-配置自动故障转移" class="headerlink" title="Configuring automatic failover(配置自动故障转移)"></a>Configuring automatic failover(配置自动故障转移)</h2><p>The configuration of automatic failover requires the addition of two new parameters to your configuration. In your hdfs-site.xml file, add:<br>自动故障转移的配置需要在配置中添加两个新参数。在你的hdfs-site.xml文件中添加:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>This specifies that the cluster should be set up for automatic failover. In your core-site.xml file, add:<br>这指定应该为自动故障转移设置集群。在你的core site.xml文件中，添加:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>This lists the host-port pairs running the ZooKeeper service.<br>列出运行ZooKeeper服务的host-port。</p>
<p>As with the parameters described earlier in the document,<br>these settings may be configured on a per-nameservice basis by suffixing the configuration key with the nameservice ID.<br>与文档前面描述的参数一样，<br>这些设置可以在每个名称服务的基础上进行配置，方法是在配置键中添加名称服务ID的后缀。</p>
<p>For example, in a cluster with federation enabled, you can explicitly enable automatic failover for only one of the nameservices by setting dfs.ha.automatic-failover.enabled.my-nameservice-id.<br>例如，在启用了联合的集群中，可以通过设置dfs.ha.automatic-failover.enabled.my-nameservice-id，显式地仅为其中一个命名服务启用自动故障转移。</p>
<p>There are also several other configuration parameters which may be set to control the behavior of automatic failover;<br>however, they are not necessary for most installations.<br>Please refer to the configuration key specific documentation for details.<br>还有一些其他的配置参数可以被设置来控制自动故障转移的行为;<br>但是，对于大多数安装来说，它们不是必需的。<br>请参阅配置密钥特定文档的详细信息。</p>
<h2 id="Initializing-HA-state-in-ZooKeeper-正在初始化ZooKeeper的HA状态"><a href="#Initializing-HA-state-in-ZooKeeper-正在初始化ZooKeeper的HA状态" class="headerlink" title="Initializing HA state in ZooKeeper(正在初始化ZooKeeper的HA状态)"></a>Initializing HA state in ZooKeeper(正在初始化ZooKeeper的HA状态)</h2><p>After the configuration keys have been added, the next step is to initialize required state in ZooKeeper.<br>配置密钥添加完成后，下一步是初始化ZooKeeper中所需的状态。</p>
<p>You can do so by running the following command from one of the NameNode hosts.<br>可以通过在一个NameNode主机上运行以下命令来实现。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[hdfs]$</span><span class="bash"> <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs zkfc -formatZK</span></span><br></pre></td></tr></table></figure>

<p>This will create a znode in ZooKeeper inside of which the automatic failover system stores its data.<br>这将在ZooKeeper中创建一个znode，自动故障转移系统将数据存储在这个znode中。</p>
<h2 id="Starting-the-cluster-with-start-dfs-sh-使用start-dfs-sh启动集群"><a href="#Starting-the-cluster-with-start-dfs-sh-使用start-dfs-sh启动集群" class="headerlink" title="Starting the cluster with start-dfs.sh(使用start-dfs.sh启动集群)"></a>Starting the cluster with start-dfs.sh(使用start-dfs.sh启动集群)</h2><p>Since automatic failover has been enabled in the configuration,<br>the start-dfs.sh script will now automatically start a ZKFC daemon on any machine that runs a NameNode.<br>由于在配置中启用了自动故障转移，<br>start-dfs.sh脚本现在将在任何运行NameNode的机器上自动启动ZKFC守护进程。</p>
<p>When the ZKFCs start, they will automatically select one of the NameNodes to become active.<br>当ZKFCs启动时，它们将自动选择一个NN使其成为active状态的。</p>
<h2 id="Starting-the-cluster-manually-手动启动集群"><a href="#Starting-the-cluster-manually-手动启动集群" class="headerlink" title="Starting the cluster manually(手动启动集群)"></a>Starting the cluster manually(手动启动集群)</h2><p>If you manually manage the services on your cluster, you will need to manually start the zkfc daemon on each of the machines that runs a NameNode.<br>You can start the daemon by running:<br>如果手动管理集群上的服务，则需要在运行NameNode的每台机器上手动启动zkfc守护进程。<br>你可以通过运行以下命令启动守护进程:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[hdfs]$</span><span class="bash"> <span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh --script <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs start zkfc</span></span><br></pre></td></tr></table></figure>


<h2 id="Securing-access-to-ZooKeeper-确保对ZooKeeper的访问"><a href="#Securing-access-to-ZooKeeper-确保对ZooKeeper的访问" class="headerlink" title="Securing access to ZooKeeper(确保对ZooKeeper的访问)"></a>Securing access to ZooKeeper(确保对ZooKeeper的访问)</h2><p>If you are running a secure cluster, you will likely want to ensure that the information stored in ZooKeeper is also secured.<br>如果您运行的是安全集群，您可能希望确保存储在ZooKeeper中的信息也是安全的。</p>
<p>This prevents malicious clients from modifying the metadata in ZooKeeper or potentially triggering a false failover.<br>这可以防止恶意客户端修改ZooKeeper中的元数据或触发错误的故障转移。</p>
<p>In order to secure the information in ZooKeeper, first add the following to your core-site.xml file:<br>为了确保ZooKeeper中的信息安全，首先在你的core site.xml文件中添加以下内容:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>@/path/to/zk-auth.txt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.acl<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>@/path/to/zk-acl.txt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Please note the ‘@’ character in these values – this specifies that the configurations are not inline, but rather point to a file on disk.<br>请注意这些值中的’ @ ‘字符 - 这表示这些配置不是内联的，而是指向磁盘上的一个文件。</p>
<p>The first configured file specifies a list of ZooKeeper authentications, in the same format as used by the ZK CLI.<br>For example, you may specify something like:<br>第一个配置文件指定了一个ZooKeeper认证列表，格式与ZK CLI使用的相同。<br>例如，你可以这样指定:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digest:hdfs-zkfcs:mypassword</span><br></pre></td></tr></table></figure>

<p>…where hdfs-zkfcs is a unique username for ZooKeeper, and mypassword is some unique string used as a password.<br>其中hdfs-zkfcs是ZooKeeper的唯一用户名，mypassword是作为密码的唯一字符串。</p>
<p>Next, generate a ZooKeeper ACL that corresponds to this authentication, using a command like the following:<br>接下来，使用如下命令生成与此身份验证对应的ZooKeeper ACL:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[hdfs]$</span><span class="bash"> java -cp <span class="variable">$ZK_HOME</span>/lib/*:<span class="variable">$ZK_HOME</span>/zookeeper-3.4.2.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:mypassword</span></span><br><span class="line">output: hdfs-zkfcs:mypassword-&gt;hdfs-zkfcs:P/OQvnYyU/nF/mGYvB/xurX8dYs=</span><br></pre></td></tr></table></figure>

<p>Copy and paste the section of this output after the ‘-&gt;’ string into the file zk-acls.txt, prefixed by the string “digest:”. For example:<br>将输出的部分复制并粘贴到字符串’ -&gt; ‘之后的zk-acl文件中。以字符串” digest: “作为前缀。例如:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digest:hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=:rwcda</span><br></pre></td></tr></table></figure>

<p>In order for these ACLs to take effect, you should then rerun the zkfc -formatZK command as described above.<br>为了使这些acl生效，您应该如上所述重新运行zkfc -formatZK命令。</p>
<p>After doing so, you may verify the ACLs from the ZK CLI as follows:<br>这样做之后，您可以验证acl从ZK CLI如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] getAcl /hadoop-ha</span><br><span class="line">&#x27;digest,&#x27;hdfs-zkfcs:vlUvLnd8MlacsE80rDuu6ONESbM=</span><br><span class="line">: cdrwa</span><br></pre></td></tr></table></figure>

<h2 id="Verifying-automatic-failover-验证自动故障转移"><a href="#Verifying-automatic-failover-验证自动故障转移" class="headerlink" title="Verifying automatic failover(验证自动故障转移)"></a>Verifying automatic failover(验证自动故障转移)</h2><p>Once automatic failover has been set up, you should test its operation.<br>设置好自动故障转移后，您应该测试它的操作。</p>
<p>To do so, first locate the active NameNode.<br>You can tell which node is active by visiting the NameNode web interfaces – each node reports its HA state at the top of the page.<br>为此，首先定位active NN。<br>您可以通过访问NameNode web接口来判断哪个节点是活动的 — 每个节点在页面顶部报告其HA状态。</p>
<p>Once you have located your active NameNode, you may cause a failure on that node.<br>一旦您定位了您的active NN，您可能会在该节点上导致故障。</p>
<p>For example, you can use kill -9 <pid of NN> to simulate a JVM crash.<br>Or, you could power cycle the machine or unplug its network interface to simulate a different kind of outage.<br>例如，可以使用kill -9 NN的PID来模拟JVM崩溃。<br>或者，您可以为机器充电，或者拔掉它的网络接口，以模拟另一种中断。</p>
<p>After triggering the outage you wish to test, the other NameNode should automatically become active within several seconds.<br>在触发您想要测试的中断后，其他NameNode应该在几秒钟内自动变为active状态。</p>
<p>The amount of time required to detect a failure and trigger a fail-over depends on the configuration of ha.zookeeper.session-timeout.ms,<br>but defaults to 5 seconds.<br>检测故障和触发故障转移所需的时间取决于 ha.zookeeper.session-timeout.ms的配置。但默认为5秒。</p>
<p>If the test does not succeed, you may have a misconfiguration.<br>如果测试不成功，则可能是配置错误。</p>
<p>Check the logs for the zkfc daemons as well as the NameNode daemons in order to further diagnose the issue.<br>检查zkfc守护进程和NameNode守护进程的日志，以便进一步诊断问题。</p>
<h1 id="Automatic-Failover-FAQ-自动故障转移常见问题解答"><a href="#Automatic-Failover-FAQ-自动故障转移常见问题解答" class="headerlink" title="Automatic Failover FAQ(自动故障转移常见问题解答)"></a>Automatic Failover FAQ(自动故障转移常见问题解答)</h1><p>· Is it important that I start the ZKFC and NameNode daemons in any particular order?<br>· 我以任何特定的顺序启动ZKFC和NameNode守护进程是否重要？<br>  No. On any given node you may start the ZKFC before or after its corresponding NameNode.<br>  不重要。在任何给定的节点上，可以在对应的NameNode之前或之后启动ZKFC。</p>
<p>· What additional monitoring should I put in place?<br>· 我应该进行哪些额外的监测?<br>  You should add monitoring on each host that runs a NameNode to ensure that the ZKFC remains running.<br>  In some types of ZooKeeper failures, for example, the ZKFC may unexpectedly exit,<br>  and should be restarted to ensure that the system is ready for automatic failover.<br>  Additionally, you should monitor each of the servers in the ZooKeeper quorum. If ZooKeeper crashes, then automatic failover will not function.<br>  您应该在每个运行NameNode的主机上添加监控，以确保ZKFC保持运行。<br>  在某些类型的ZooKeeper故障中，例如，ZKFC可能意外退出，<br>  并且应该重新启动，以确保系统为自动故障转移做好准备。<br>  此外，你应该监视ZooKeeper quorum中的每个服务器。如果ZooKeeper崩溃，自动故障转移将无法正常工作。</p>
<p>· What happens if ZooKeeper goes down?<br>· 如果ZooKeeper down了怎么办?<br>  If the ZooKeeper cluster crashes, no automatic failovers will be triggered.<br>  However, HDFS will continue to run without any impact. When ZooKeeper is restarted, HDFS will reconnect with no issues.<br>  当ZooKeeper集群崩溃时，不会触发自动故障切换。<br>  但是，HDFS将继续运行，不会受到任何影响。重启ZooKeeper后，HDFS会重新连接，没有问题。</p>
<p>· Can I designate one of my NameNodes as primary/preferred?<br>· 我可以指定我的NameNodes之一为主/首选吗?<br>  No. Currently, this is not supported. Whichever NameNode is started first will become active.<br>  You may choose to start the cluster in a specific order such that your preferred node starts first.<br>  不。目前，这是不支持的。首先启动的NameNode将被激活。<br>  您可以选择以特定的顺序启动集群，这样您首选的节点就会首先启动。</p>
<p>· How can I initiate a manual failover when automatic failover is configured?<br>· 配置了自动故障转移后，如何启动手动故障转移?<br>  Even if automatic failover is configured, you may initiate a manual failover using the same hdfs haadmin command.<br>  It will perform a coordinated failover.<br>  即使配置了自动故障切换，也可以使用相同的hdfs haadmin命令手动启动故障切换。<br>  它将执行协调的故障转移。</p>
<h1 id="HDFS-Upgrade-Finalization-Rollback-with-HA-Enabled-开启HA功能的HDFS升级-结束-回滚"><a href="#HDFS-Upgrade-Finalization-Rollback-with-HA-Enabled-开启HA功能的HDFS升级-结束-回滚" class="headerlink" title="HDFS Upgrade/Finalization/Rollback with HA Enabled(开启HA功能的HDFS升级/结束/回滚)"></a>HDFS Upgrade/Finalization/Rollback with HA Enabled(开启HA功能的HDFS升级/结束/回滚)</h1><p>When moving between versions of HDFS, sometimes the newer software can simply be installed and the cluster restarted.<br>当在不同版本的HDFS之间移动时，有时可以简单地安装新软件，然后重新启动集群。</p>
<p>Sometimes, however, upgrading the version of HDFS you’re running may require changing on-disk data.<br>然而，有时升级正在运行的HDFS版本可能需要更改磁盘上的数据。</p>
<p>In this case, one must use the HDFS Upgrade/Finalize/Rollback facility after installing the new software.<br>在这种情况下，必须在安装新软件后使用HDFS的Upgrade/Finalize/Rollback工具。</p>
<p>This process is made more complex in an HA environment, since the on-disk metadata that the NN relies upon is by definition distributed,<br>both on the two HA NNs in the pair, and on the JournalNodes in the case that QJM is being used for the shared edits storage.<br>在HA环境中，这个过程更加复杂，因为NN所依赖的磁盘元数据根据定义是分布式的，<br>在该对中的两个HA nn上，以及在QJM被用于共享编辑存储的情况下在journalnode上。</p>
<p>This documentation section describes the procedure to use the HDFS Upgrade/Finalize/Rollback facility in an HA setup.<br>本文档介绍了在HA设置中使用HDFS升级/Finalize/Rollback功能的过程。</p>
<p><strong>To perform an HA upgrade</strong>, the operator must do the following:<br><strong>如果需要进行HA升级</strong>，需要执行以下操作:</p>
<p>1.Shut down all of the NNs as normal, and install the newer software.<br>1.正常关闭所有的网络服务网络，并安装更新的软件。</p>
<p>2.Start up all of the JNs. Note that it is critical that all the JNs be running when performing the upgrade, rollback, or finalization operations.<br>If any of the JNs are down at the time of running any of these operations, the operation will fail.<br>2.启动所有的JNs。请注意，在执行升级、回滚或结束操作时，必须运行所有的JNs。<br>如果任何JNs在运行这些操作时关闭，则操作将失败。</p>
<p>3.Start one of the NNs with the ‘-upgrade’ flag.<br>3.启动一个带有’-upgrade’标志的NN。</p>
<p>4.On start, this NN will not enter the standby state as usual in an HA setup. Rather, this NN will immediately enter the active state,<br>perform an upgrade of its local storage dirs, and also perform an upgrade of the shared edit log.<br>4.在开始时，这个NN将不会像往常一样在HA设置中进入standby状态。相反，这个神经网络会立即进入active状态，<br>对其本地存储目录进行升级，并对共享编辑日志进行升级。</p>
<p>5.At this point the other NN in the HA pair will be out of sync with the upgraded NN.<br>In order to bring it back in sync and once again have a highly available setup, you should re-bootstrap this NameNode by running the NN with the ‘-bootstrapStandby’ flag.<br>It is an error to start this second NN with the ‘-upgrade’ flag.<br>5.此时，HA对中的另一个NN将与升级后的NN不同步。<br>为了让它恢复同步并再次获得一个高度可用的设置，您应该通过运行带有’-bootstrapStandby’标志的NN来重新引导这个NameNode。<br>用’-upgrade’标志开始第二个NN是错误的。</p>
<p>Note that if at any time you want to restart the NameNodes before finalizing or rolling back the upgrade,<br>you should start the NNs as normal, i.e. without any special startup flag.<br>请注意，如果您想在完成或回滚升级之前重启namenode，<br>你应该像正常的那样启动NNs，即不需要任何特殊的启动标志。</p>
<p><strong>To finalize an HA upgrade</strong>, the operator will use the ‘hdfs dfsadmin -finalizeUpgrade’ command while the NNs are running and one of them is active.<br>The active NN at the time this happens will perform the finalization of the shared log,<br>and the NN whose local storage directories contain the previous FS state will delete its local state.<br><strong>完成一个HA升级</strong>,当NNs运行且其中一个处于active状态时，操作员将使用“hdfs dfsadmin -finalizeUpgrade”命令。<br>发生这种情况时，active NN将执行共享日志的结束，其本地存储目录包含前一个FS状态的NN将删除其本地状态。</p>
<p><strong>To perform a rollback</strong> of an upgrade, both NNs should first be shut down.<br>The operator should run the roll back command on the NN where they initiated the upgrade procedure,<br>which will perform the rollback on the local dirs there, as well as on the shared log,either NFS or on the JNs.<br>Afterward, this NN should be started and the operator should run ‘-bootstrapStandby’ on the other NN to bring the two NNs in sync with this rolled-back file system state.<br><strong>要回滚</strong>升级，两个NNs都应该先关闭。<br>操作员应该在启动升级过程的NN上运行回滚命令，<br>它将在本地目录和共享日志上执行回滚，NFS或JNs上。<br>然后，这个神经网络应该被启动，操作员应该在另一个神经网络上运行’-bootstrapStandby’，使两个神经网络与这个回滚的文件系统状态同步。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">田一顷</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2021/01/15/HDFS-HA/">http://example.com/2021/01/15/HDFS-HA/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">my precious</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/01/18/%E5%9F%BA%E4%BA%8EScala%E7%BC%96%E5%86%99%E7%9A%84%E5%87%A0%E4%B8%AASpark%E5%85%A5%E9%97%A8%E5%B0%8F%E7%A8%8B%E5%BA%8F/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">基于Scala编写的几个Spark入门小程序</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/29/python_learn/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Python基础知识</div></div></a></div></nav></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/header.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">田一顷</div><div class="author-info__description">田一顷</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E8%AF%8D"><span class="toc-number">1.</span> <span class="toc-text">生词</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Purpose-%E7%9B%AE%E6%A0%87"><span class="toc-number">2.</span> <span class="toc-text">Purpose(目标)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Note-Using-the-Quorum-Journal-Manager-or-Conventional-Shared-Storage-%E6%B3%A8%E6%84%8F%EF%BC%9A%E4%BD%BF%E7%94%A8QJM%E6%88%96%E8%80%85%E5%B8%B8%E8%A7%84%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8"><span class="toc-number">3.</span> <span class="toc-text">Note: Using the Quorum Journal Manager or Conventional Shared Storage(注意：使用QJM或者常规共享存储)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Background-%E8%83%8C%E6%99%AF"><span class="toc-number">4.</span> <span class="toc-text">Background(背景)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Architecture-%E7%BB%93%E6%9E%84"><span class="toc-number">5.</span> <span class="toc-text">Architecture(结构)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hardware-resources-%E7%A1%AC%E4%BB%B6%E8%B5%84%E6%BA%90"><span class="toc-number">6.</span> <span class="toc-text">Hardware resources(硬件资源)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deployment-%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F"><span class="toc-number">7.</span> <span class="toc-text">Deployment(部署方式)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Configuration-overview-%E9%85%8D%E7%BD%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">7.1.</span> <span class="toc-text">Configuration overview(配置概述)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Configuration-details-%E9%85%8D%E7%BD%AE%E7%BB%86%E8%8A%82"><span class="toc-number">7.2.</span> <span class="toc-text">Configuration details(配置细节)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deployment-details-%E9%83%A8%E7%BD%B2%E7%BB%86%E8%8A%82"><span class="toc-number">7.3.</span> <span class="toc-text">Deployment details(部署细节)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Administrative-commands-%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4"><span class="toc-number">7.4.</span> <span class="toc-text">Administrative commands(管理命令)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Load-Balancer-Setup-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8%E8%AE%BE%E7%BD%AE"><span class="toc-number">7.5.</span> <span class="toc-text">Load Balancer Setup(负载均衡器设置)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#In-Progress-Edit-Log-Tailing-%E6%AD%A3%E5%9C%A8%E8%BF%9B%E8%A1%8C%E7%9A%84%E7%BC%96%E8%BE%91%E6%97%A5%E5%BF%97%E5%B0%BE"><span class="toc-number">7.6.</span> <span class="toc-text">In-Progress Edit Log Tailing(正在进行的编辑日志尾)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Automatic-Failover-%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">8.</span> <span class="toc-text">Automatic Failover(自动故障转移)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-%E4%BB%8B%E7%BB%8D"><span class="toc-number">8.1.</span> <span class="toc-text">Introduction(介绍)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Components-%E7%BB%84%E4%BB%B6"><span class="toc-number">8.2.</span> <span class="toc-text">Components(组件)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deploying-ZooKeeper-%E9%83%A8%E7%BD%B2ZooKeeper"><span class="toc-number">8.3.</span> <span class="toc-text">Deploying ZooKeeper(部署ZooKeeper)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Before-you-begin-%E5%9C%A8%E4%BD%A0%E5%BC%80%E5%A7%8B%E4%B9%8B%E5%89%8D"><span class="toc-number">8.4.</span> <span class="toc-text">Before you begin(在你开始之前)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Configuring-automatic-failover-%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">8.5.</span> <span class="toc-text">Configuring automatic failover(配置自动故障转移)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Initializing-HA-state-in-ZooKeeper-%E6%AD%A3%E5%9C%A8%E5%88%9D%E5%A7%8B%E5%8C%96ZooKeeper%E7%9A%84HA%E7%8A%B6%E6%80%81"><span class="toc-number">8.6.</span> <span class="toc-text">Initializing HA state in ZooKeeper(正在初始化ZooKeeper的HA状态)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Starting-the-cluster-with-start-dfs-sh-%E4%BD%BF%E7%94%A8start-dfs-sh%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">8.7.</span> <span class="toc-text">Starting the cluster with start-dfs.sh(使用start-dfs.sh启动集群)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Starting-the-cluster-manually-%E6%89%8B%E5%8A%A8%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">8.8.</span> <span class="toc-text">Starting the cluster manually(手动启动集群)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Securing-access-to-ZooKeeper-%E7%A1%AE%E4%BF%9D%E5%AF%B9ZooKeeper%E7%9A%84%E8%AE%BF%E9%97%AE"><span class="toc-number">8.9.</span> <span class="toc-text">Securing access to ZooKeeper(确保对ZooKeeper的访问)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Verifying-automatic-failover-%E9%AA%8C%E8%AF%81%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">8.10.</span> <span class="toc-text">Verifying automatic failover(验证自动故障转移)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Automatic-Failover-FAQ-%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94"><span class="toc-number">9.</span> <span class="toc-text">Automatic Failover FAQ(自动故障转移常见问题解答)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS-Upgrade-Finalization-Rollback-with-HA-Enabled-%E5%BC%80%E5%90%AFHA%E5%8A%9F%E8%83%BD%E7%9A%84HDFS%E5%8D%87%E7%BA%A7-%E7%BB%93%E6%9D%9F-%E5%9B%9E%E6%BB%9A"><span class="toc-number">10.</span> <span class="toc-text">HDFS Upgrade&#x2F;Finalization&#x2F;Rollback with HA Enabled(开启HA功能的HDFS升级&#x2F;结束&#x2F;回滚)</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/02/27/HDFS%20API/" title="HDFS API"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HDFS API"/></a><div class="content"><a class="title" href="/2021/02/27/HDFS%20API/" title="HDFS API">HDFS API</a><time datetime="2021-02-27T03:27:42.337Z" title="发表于 2021-02-27 11:27:42">2021-02-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/24/Shell%E8%BF%9B%E9%98%B6-%E7%BB%83%E4%B9%A0%E9%A2%98/" title="Shell进阶-练习题"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Shell进阶-练习题"/></a><div class="content"><a class="title" href="/2021/02/24/Shell%E8%BF%9B%E9%98%B6-%E7%BB%83%E4%B9%A0%E9%A2%98/" title="Shell进阶-练习题">Shell进阶-练习题</a><time datetime="2021-02-24T14:26:46.150Z" title="发表于 2021-02-24 22:26:46">2021-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/09/Scala%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0/" title="Scala高阶函数"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Scala高阶函数"/></a><div class="content"><a class="title" href="/2021/02/09/Scala%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0/" title="Scala高阶函数">Scala高阶函数</a><time datetime="2021-02-08T16:44:16.676Z" title="发表于 2021-02-09 00:44:16">2021-02-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/08/LeetCode/" title="LeetCode"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode"/></a><div class="content"><a class="title" href="/2021/02/08/LeetCode/" title="LeetCode">LeetCode</a><time datetime="2021-02-08T12:56:34.021Z" title="发表于 2021-02-08 20:56:34">2021-02-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/22/Scala%E5%87%BD%E6%95%B0%E4%B8%8E%E6%96%B9%E6%B3%95/" title="Scala函数"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Scala函数"/></a><div class="content"><a class="title" href="/2021/01/22/Scala%E5%87%BD%E6%95%B0%E4%B8%8E%E6%96%B9%E6%B3%95/" title="Scala函数">Scala函数</a><time datetime="2021-01-22T10:53:11.431Z" title="发表于 2021-01-22 18:53:11">2021-01-22</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 田一顷</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>