<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>sqoop安装使用 | my precious</title><meta name="author" content="田一顷"><meta name="copyright" content="田一顷"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Sqoop在关系型数据库和Hadoop之间建立了一个桥梁，让我们更方便的导入导出数据Sqoop的底层是使用MR来完成的 安装下载http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;cdh&#x2F;5&#x2F;搜索：sqoop-1.4.6-cdh5.16.2.tar.gz下载好之后上传到自己的云主机不管是用rz命令还是其他工具，如winscp压缩包最好都放在一起，方便查找我放在了&#x2F;home&#x2F;hadoo">
<meta property="og:type" content="article">
<meta property="og:title" content="sqoop安装使用">
<meta property="og:url" content="http://example.com/2020/12/17/sqoop/index.html">
<meta property="og:site_name" content="my precious">
<meta property="og:description" content="Sqoop在关系型数据库和Hadoop之间建立了一个桥梁，让我们更方便的导入导出数据Sqoop的底层是使用MR来完成的 安装下载http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;cdh&#x2F;5&#x2F;搜索：sqoop-1.4.6-cdh5.16.2.tar.gz下载好之后上传到自己的云主机不管是用rz命令还是其他工具，如winscp压缩包最好都放在一起，方便查找我放在了&#x2F;home&#x2F;hadoo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2020-12-17T11:45:04.515Z">
<meta property="article:modified_time" content="2020-12-17T14:54:50.143Z">
<meta property="article:author" content="田一顷">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2020/12/17/sqoop/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-12-17 22:54:50'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/header.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div></div></div><hr/></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">my precious</a></span><span id="menus"><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">sqoop安装使用</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-12-17T11:45:04.515Z" title="发表于 2020-12-17 19:45:04">2020-12-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-12-17T14:54:50.143Z" title="更新于 2020-12-17 22:54:50">2020-12-17</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Sqoop在关系型数据库和Hadoop之间建立了一个桥梁，让我们更方便的导入导出数据<br>Sqoop的底层是使用MR来完成的</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p><a target="_blank" rel="noopener" href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a><br>搜索：sqoop-1.4.6-cdh5.16.2.tar.gz<br>下载好之后上传到自己的云主机<br>不管是用rz命令还是其他工具，如winscp<br>压缩包最好都放在一起，方便查找<br>我放在了/home/hadoop/software，包括之前安装hadoop和hive都在这个文件夹</p>
<h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /home/hadoop/software</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf sqoop-1.4.6-cdh5.16.2.tar.gz -C /home/hadoop/app/</span></span><br></pre></td></tr></table></figure>

<h2 id="创建软连接"><a href="#创建软连接" class="headerlink" title="创建软连接"></a>创建软连接</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /home/hadoop/app</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ln -s sqoop-1.4.6-cdh5.16.2/ sqoop</span></span><br></pre></td></tr></table></figure>

<h2 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h2><h3 id="sqoop-env-sh"><a href="#sqoop-env-sh" class="headerlink" title="sqoop-env.sh"></a>sqoop-env.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /home/hadoop/app/sqoop/conf</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp sqoop-env-template.sh sqoop-env.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi sqoop-env.sh</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hive</span><br></pre></td></tr></table></figure>
<p>文件内容本身是有#注释的，解开并填上自己对应的hadoop和hive地址即可，如果有HBASE或ZK，对应添加</p>
<h3 id="bashrc"><a href="#bashrc" class="headerlink" title=".bashrc"></a>.bashrc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi .bashrc</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> .bashrc	【别忘记让环境变量生效】</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SQOOP_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;sqoop</span><br><span class="line">export PATH&#x3D;$&#123;SQOOP_HOME&#125;&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>

<h2 id="添加驱动"><a href="#添加驱动" class="headerlink" title="添加驱动"></a>添加驱动</h2><p>因为要用MySQL和Hive/HDFS进行数据导入导出，所以这里需要用到MySQL驱动包，之前Hive安装中也用到过</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp mysql-connector-java-5.1.47.jar /home/hadoop/app/sqoop/lib/</span></span><br></pre></td></tr></table></figure>

<h2 id="检查是否安装成功"><a href="#检查是否安装成功" class="headerlink" title="检查是否安装成功"></a>检查是否安装成功</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sqoop version</span></span><br></pre></td></tr></table></figure>
<p>出现Warning不要紧张，是因为没有配置HBASE，ZK等<br>出现 Sqoop 1.4.6-cdh5.16.2 则表示安装成功</p>
<h1 id="官方学习文档-超详细"><a href="#官方学习文档-超详细" class="headerlink" title="官方学习文档(超详细)"></a>官方学习文档(超详细)</h1><p><a target="_blank" rel="noopener" href="http://sqoop.apache.org/docs/1.4.6/index.html">http://sqoop.apache.org/docs/1.4.6/index.html</a><br>sqoop的学习文档非常详细，因为安装的是1.4.6版本，所以用这一份文档来学习</p>
<h2 id="sqoop-help"><a href="#sqoop-help" class="headerlink" title="sqoop help"></a>sqoop help</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records	生成代码以与数据库记录进行交互</span><br><span class="line">  create-hive-table  Import a table definition into Hive				将表定义导入到Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results	评估一条SQL语句并显示结果</span><br><span class="line">  export             Export an HDFS directory to a database table		将HDFS文件导出到数据库表</span><br><span class="line">  help               List available commands							列出可用的命令</span><br><span class="line">  import             Import a table from a database to HDFS				将表从数据库导入到HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS				将所有表从数据库导入到HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS	将数据集从大型机服务器导入HDFS</span><br><span class="line">  job                Work with saved jobs								处理已保存的作业</span><br><span class="line">  list-databases     List available databases on a server				列出服务器上的可用数据库</span><br><span class="line">  list-tables        List available tables in a database				列出数据库中的可用表</span><br><span class="line">  merge              Merge results of incremental imports				合并增量进口的结果</span><br><span class="line">  metastore          Run a standalone Sqoop metastore					运行独立的Sqoop元存储</span><br><span class="line">  version            Display version information						显示版本信息</span><br></pre></td></tr></table></figure>

<p>sqoop help 上述任何一个参数就可以得到非常详细的使用文档，详细到夸张</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="list-databases-amp-list-tables"><a href="#list-databases-amp-list-tables" class="headerlink" title="list-databases &amp; list-tables"></a>list-databases &amp; list-tables</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line">--connect jdbc:mysql://localhost:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;password&#x27;</span><br></pre></td></tr></table></figure>
<p>列出可用的数据库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious_hive \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;password&#x27;</span><br></pre></td></tr></table></figure>
<p>列出指定数据库中可用的表</p>
<h2 id="import"><a href="#import" class="headerlink" title="import"></a>import</h2><h3 id="MySQL-RDBMS-gt-HDFS"><a href="#MySQL-RDBMS-gt-HDFS" class="headerlink" title="MySQL(RDBMS) ==&gt; HDFS"></a>MySQL(RDBMS) ==&gt; HDFS</h3><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;Tyq1995222.&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--columns &quot;name,gender&quot; \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2HDFS \</span><br><span class="line">--table emp \</span><br><span class="line">--fields-terminated-by &#x27;,&#x27; \</span><br><span class="line">--where &#x27;id&lt;4&#x27; \</span><br><span class="line">--target-dir EMP</span><br></pre></td></tr></table></figure>

<p>执行报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20&#x2F;12&#x2F;15 16:32:19 ERROR tool.ImportTool: Import failed: No primary key could be found for table emp. Please specify one with --split-by or perform a sequential import with &#39;-m 1&#39;.</span><br></pre></td></tr></table></figure>
<p>意思是，emp表没有主键，请使用–split-by指定一个，或使用’-m 1’执行顺序导入，我这里使用’-m 1’</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;password&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--columns &quot;name,gender&quot; \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2HDFS \</span><br><span class="line">--table emp \</span><br><span class="line">--fields-terminated-by &#x27;,&#x27; \</span><br><span class="line">--where &#x27;id&lt;4&#x27; \</span><br><span class="line">--target-dir EMP \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>
<p>再次执行出现了第二个报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;json&#x2F;JSONObject</span><br><span class="line">......</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.json.JSONObject</span><br></pre></td></tr></table></figure>
<p>这里看到是缺少json的jar包，去maven下一个吧<br><a target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/org.json/json">https://mvnrepository.com/artifact/org.json/json</a><br>下载安装后放在sqoop的lib目录中</p>
<p>再次执行，成功，文件默认是存放在HDFS的**/user/hadoop**下，我们来看一下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -ls /user/hadoop/EMP</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r--   1 hadoop supergroup          0 2020-12-15 16:54 &#x2F;user&#x2F;hadoop&#x2F;EMP&#x2F;_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         21 2020-12-15 16:54 &#x2F;user&#x2F;hadoop&#x2F;EMP&#x2F;part-m-00000</span><br></pre></td></tr></table></figure>
<p>可以看到是成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -cat /user/hadoop/EMP/part-m-00000</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">andy,1</span><br><span class="line">bob,1</span><br><span class="line">cindy,2</span><br></pre></td></tr></table></figure>
<p>可以看到数据也进来了，并且符合我们的要求<br>拐回去看一下sqoop执行的日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20&#x2F;12&#x2F;15 16:47:38 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM &#96;emp&#96; AS t LIMIT 1</span><br></pre></td></tr></table></figure>
<p>这里发现sqoop会在读MySQL之前看一下表是否存在，若表不存在，会报错</p>
<p>如果有表关联或者有很复杂的逻辑在其中，那么可以使用方法二，使用-e参数，将SQL的结果放入HDFS</p>
<h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;password&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2HDFS_QUERY \</span><br><span class="line">--fields-terminated-by &#x27;|&#x27; \</span><br><span class="line">-e &quot;select * from emp where id &lt; 4 and \$CONDITIONS&quot; \</span><br><span class="line">--target-dir EMP_QUERY</span><br></pre></td></tr></table></figure>
<p>执行报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">When importing query results in parallel, you must specify --split-by.</span><br></pre></td></tr></table></figure>
<p>看来使用SQL查询必须要有一个–split-by，这里就用id吧</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;password&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2HDFS_QUERY \</span><br><span class="line">--fields-terminated-by &#x27;|&#x27; \</span><br><span class="line">-e &quot;select * from emp where id &lt; 4 and \$CONDITIONS&quot; \</span><br><span class="line">--target-dir EMP_QUERY \</span><br><span class="line">--split-by id</span><br></pre></td></tr></table></figure>
<p>再次执行，成功，看一下HDFS</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -ls /user/hadoop/EMP_QUERY</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r--   1 hadoop supergroup          0 2020-12-15 17:12 &#x2F;user&#x2F;hadoop&#x2F;EMP_QUERY&#x2F;_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         12 2020-12-15 17:11 &#x2F;user&#x2F;hadoop&#x2F;EMP_QUERY&#x2F;part-m-00000</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2020-12-15 17:11 &#x2F;user&#x2F;hadoop&#x2F;EMP_QUERY&#x2F;part-m-00001</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         11 2020-12-15 17:12 &#x2F;user&#x2F;hadoop&#x2F;EMP_QUERY&#x2F;part-m-00002</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         13 2020-12-15 17:12 &#x2F;user&#x2F;hadoop&#x2F;EMP_QUERY&#x2F;part-m-00003</span><br></pre></td></tr></table></figure>
<p>对比第一种方式，这里除_SUCCESS之外出现了4个文件，可以想到’-m 1’的作用就是将结果放入一个文件中，并且<strong>默认是4个</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -cat /user/hadoop/EMP_QUERY/part-m-0000*</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1|andy|RD|1</span><br><span class="line">2|bob|RD|1</span><br><span class="line">3|cindy|QA|2</span><br></pre></td></tr></table></figure>
<p>结果就是我们想要的格式</p>
<p>试了好多次，发现每次总是part-m-00001没有数据，暂不清楚其中的机制</p>
<h3 id="MySQL-RDBMS-gt-Hive"><a href="#MySQL-RDBMS-gt-Hive" class="headerlink" title="MySQL(RDBMS) ==&gt; Hive"></a>MySQL(RDBMS) ==&gt; Hive</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;Tyq1995222.&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-database precious_test \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table emp_hive \</span><br><span class="line">--columns &quot;id,name,dept,gender&quot; \</span><br><span class="line">--mapreduce-job-name EmpFromMySQL2Hive \</span><br><span class="line">--table emp \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>
<p>执行报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.</span><br></pre></td></tr></table></figure>
<p>百度查到是缺少jar包<br>可以将hive/lib下的hive-exec-1.1.0-cdh5.16.2.jar拷贝到sqoop/lib下，sqoop怎么缺这么多jar包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /home/hadoop/app/hive/lib</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp hive-exec-1.1.0-cdh5.16.2.jar /home/hadoop/app/sqoop/lib/</span></span><br></pre></td></tr></table></figure>

<p>再次执行成功，查看一下hive中是否有了这张表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">use precious_test;</span><br><span class="line">select * from emp_hive;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">id	name	dept	gender</span><br><span class="line">1	andy	RD	1</span><br><span class="line">2	bob	RD	1</span><br><span class="line">3	cindy	QA	2</span><br><span class="line">4	dandy	QA	1</span><br></pre></td></tr></table></figure>
<p>没有问题，数据进来了</p>
<h2 id="export"><a href="#export" class="headerlink" title="export"></a>export</h2><p>sqoop eport 是将数据从HDFS ==&gt; RDBMS，注意：RDBMS上的表需要已经存在</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/precious \</span><br><span class="line">--username root \</span><br><span class="line">--password &#x27;password&#x27; \</span><br><span class="line">--table emp_HDFS2MySQL \</span><br><span class="line">--export-dir /user/hadoop/EMP_QUERY \</span><br><span class="line">--input-fields-terminated-by &#x27;|&#x27; </span><br></pre></td></tr></table></figure>
<p>执行成功，去MySQL中查一下</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> precious;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp_HDFS2MySQL;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+------+-------+------+--------+</span><br><span class="line">| id   | name  | dept | gender |</span><br><span class="line">+------+-------+------+--------+</span><br><span class="line">| 3    | cindy | QA   | 2      |</span><br><span class="line">| 1    | andy  | RD   | 1      |</span><br><span class="line">| 2    | bob   | RD   | 1      |</span><br><span class="line">+------+-------+------+--------+</span><br></pre></td></tr></table></figure>
<p>可以看到数据进来了</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">田一顷</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2020/12/17/sqoop/">http://example.com/2020/12/17/sqoop/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">my precious</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2020/12/17/YARN-%E8%B0%83%E4%BC%98/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">YARN - 调优</div></div></a></div></nav></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/header.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">田一顷</div><div class="author-info__description">田一顷</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.</span> <span class="toc-text">安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD"><span class="toc-number">1.1.</span> <span class="toc-text">下载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B"><span class="toc-number">1.2.</span> <span class="toc-text">解压</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%BD%AF%E8%BF%9E%E6%8E%A5"><span class="toc-number">1.3.</span> <span class="toc-text">创建软连接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">添加环境变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sqoop-env-sh"><span class="toc-number">1.4.1.</span> <span class="toc-text">sqoop-env.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bashrc"><span class="toc-number">1.4.2.</span> <span class="toc-text">.bashrc</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E9%A9%B1%E5%8A%A8"><span class="toc-number">1.5.</span> <span class="toc-text">添加驱动</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F"><span class="toc-number">1.6.</span> <span class="toc-text">检查是否安装成功</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E5%AD%A6%E4%B9%A0%E6%96%87%E6%A1%A3-%E8%B6%85%E8%AF%A6%E7%BB%86"><span class="toc-number">2.</span> <span class="toc-text">官方学习文档(超详细)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sqoop-help"><span class="toc-number">2.1.</span> <span class="toc-text">sqoop help</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8"><span class="toc-number">3.</span> <span class="toc-text">使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#list-databases-amp-list-tables"><span class="toc-number">3.1.</span> <span class="toc-text">list-databases &amp; list-tables</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import"><span class="toc-number">3.2.</span> <span class="toc-text">import</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL-RDBMS-gt-HDFS"><span class="toc-number">3.2.1.</span> <span class="toc-text">MySQL(RDBMS) &#x3D;&#x3D;&gt; HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">方法一</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">方法二</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL-RDBMS-gt-Hive"><span class="toc-number">3.2.2.</span> <span class="toc-text">MySQL(RDBMS) &#x3D;&#x3D;&gt; Hive</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#export"><span class="toc-number">3.3.</span> <span class="toc-text">export</span></a></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/sqoop/" title="sqoop安装使用"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="sqoop安装使用"/></a><div class="content"><a class="title" href="/2020/12/17/sqoop/" title="sqoop安装使用">sqoop安装使用</a><time datetime="2020-12-17T11:45:04.515Z" title="发表于 2020-12-17 19:45:04">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/YARN-%E8%B0%83%E4%BC%98/" title="YARN - 调优"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="YARN - 调优"/></a><div class="content"><a class="title" href="/2020/12/17/YARN-%E8%B0%83%E4%BC%98/" title="YARN - 调优">YARN - 调优</a><time datetime="2020-12-17T11:44:56.007Z" title="发表于 2020-12-17 19:44:56">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/14/java/" title="java基础知识"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="java基础知识"/></a><div class="content"><a class="title" href="/2020/12/14/java/" title="java基础知识">java基础知识</a><time datetime="2020-12-14T13:23:21.346Z" title="发表于 2020-12-14 21:23:21">2020-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/14/Hive-%E5%91%BD%E4%BB%A4/" title="Hive-命令"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hive-命令"/></a><div class="content"><a class="title" href="/2020/12/14/Hive-%E5%91%BD%E4%BB%A4/" title="Hive-命令">Hive-命令</a><time datetime="2020-12-14T12:02:07.812Z" title="发表于 2020-12-14 20:02:07">2020-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/07/MapReduce%20on%20YARN%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="MapReduce on YARN 提交流程"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MapReduce on YARN 提交流程"/></a><div class="content"><a class="title" href="/2020/12/07/MapReduce%20on%20YARN%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="MapReduce on YARN 提交流程">MapReduce on YARN 提交流程</a><time datetime="2020-12-07T13:07:27.860Z" title="发表于 2020-12-07 21:07:27">2020-12-07</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 By 田一顷</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>